{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP Processing\n",
    "import nltk\n",
    "#nltk.download(\"averaged_perceptron_tagger\")\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download(\"wordnet\")\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Classification Models\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.gaussian_process import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.discriminant_analysis import *\n",
    "from sklearn.multiclass import *\n",
    "\n",
    "from math import *\n",
    "# set a random seed for validation split \n",
    "np.random.seed(42)\n",
    "\n",
    "# parallel fit for hyper-parameter optimization\n",
    "import parfit.parfit as pf\n",
    "from parfit import *\n",
    "\n",
    "# model explanation libraries\n",
    "import shap\n",
    "import lime\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data and Inital Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPnkHADveVk-"
   },
   "outputs": [],
   "source": [
    "drugs = pd.concat([pd.read_csv(\"train.tsv\",delimiter=\"\\t\"), pd.read_csv(\"test.tsv\",delimiter=\"\\t\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FcwLGyVVeVlG"
   },
   "outputs": [],
   "source": [
    "# change first column name to \"id\"\n",
    "col = list(drugs.columns)\n",
    "col[0] = \"id\"\n",
    "drugs.columns = col\n",
    "\n",
    "# feature engineer positive, neutral, and negative categories for rating.\n",
    "# 1-4 -> negative (0)\n",
    "# 5-7 -> netural (1)\n",
    "# 8-10 -> positive (2)\n",
    "drugs[\"rating_categorized\"] = drugs.rating.apply(lambda x : 2 if x > 7 else (1 if (x > 4 and x <= 7) else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "kDGnMhuZeVlL",
    "outputId": "21ce0141-cd12-4c7b-efb9-91f13b01ea8d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>rating_categorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14311</th>\n",
       "      <td>57701</td>\n",
       "      <td>Acetaminophen / hydrocodone</td>\n",
       "      <td>Pain</td>\n",
       "      <td>\"I suffered a .45 caliber GSW to the head in 2...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>June 26, 2015</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72301</th>\n",
       "      <td>168695</td>\n",
       "      <td>Vilazodone</td>\n",
       "      <td>Major Depressive Disorde</td>\n",
       "      <td>\"I started on Viibryd after having a DNA test ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>December 27, 2015</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23041</th>\n",
       "      <td>1607</td>\n",
       "      <td>Eluxadoline</td>\n",
       "      <td>Irritable Bowel Syndrome</td>\n",
       "      <td>\"Not covered by insurance\"</td>\n",
       "      <td>6.0</td>\n",
       "      <td>December 19, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36424</th>\n",
       "      <td>17559</td>\n",
       "      <td>Ethinyl estradiol / etonogestrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I previously did a review on Nuva Ring about ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 1, 2016</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133612</th>\n",
       "      <td>158793</td>\n",
       "      <td>Methylphenidate</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"My son was prescribed the drug just before su...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>September 3, 2009</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                          drugName                 condition  \\\n",
       "14311    57701       Acetaminophen / hydrocodone                      Pain   \n",
       "72301   168695                        Vilazodone  Major Depressive Disorde   \n",
       "23041     1607                       Eluxadoline  Irritable Bowel Syndrome   \n",
       "36424    17559  Ethinyl estradiol / etonogestrel             Birth Control   \n",
       "133612  158793                   Methylphenidate                      ADHD   \n",
       "\n",
       "                                                   review  rating  \\\n",
       "14311   \"I suffered a .45 caliber GSW to the head in 2...     9.0   \n",
       "72301   \"I started on Viibryd after having a DNA test ...     1.0   \n",
       "23041                          \"Not covered by insurance\"     6.0   \n",
       "36424   \"I previously did a review on Nuva Ring about ...     8.0   \n",
       "133612  \"My son was prescribed the drug just before su...     6.0   \n",
       "\n",
       "                     date  usefulCount  rating_categorized  \n",
       "14311       June 26, 2015           10                   2  \n",
       "72301   December 27, 2015            1                   0  \n",
       "23041   December 19, 2016            0                   1  \n",
       "36424    November 1, 2016            8                   2  \n",
       "133612  September 3, 2009           17                   1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drugs.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove HTML numeric codes\n",
    "drugs[\"review\"] = list(map(lambda x : x.replace(\"&#039;\", \"\").replace(\"&quot\",\"\"), drugs.review))\n",
    "\n",
    "# use a smaller subset of the data for quicker processing\n",
    "data = drugs[:10000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with NaN values\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change all reviews to lowercase\n",
    "data[\"review\"] = [review.lower() for review in data.review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize each word\n",
    "data[\"tk\"] = data.review.apply(lambda x : word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>rating_categorized</th>\n",
       "      <th>tk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7554</th>\n",
       "      <td>2855</td>\n",
       "      <td>Acetaminophen / pamabrom</td>\n",
       "      <td>Back Pain</td>\n",
       "      <td>\"i found this while looking for acetaminophen ...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>February 13, 2017</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>[``, i, found, this, while, looking, for, acet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4679</th>\n",
       "      <td>116091</td>\n",
       "      <td>OnabotulinumtoxinA</td>\n",
       "      <td>Overactive Bladde</td>\n",
       "      <td>\"had the botox for overactive bladder november...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>December 23, 2015</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>[``, had, the, botox, for, overactive, bladder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>44044</td>\n",
       "      <td>Ethinyl estradiol / norgestimate</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"i switched from loestrin 24 fe to ortho tri-c...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>January 11, 2012</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[``, i, switched, from, loestrin, 24, fe, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>169167</td>\n",
       "      <td>Zelapar</td>\n",
       "      <td>Parkinson's Disease</td>\n",
       "      <td>\"i had to stop taking zelapar because i was ha...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>January 19, 2009</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>[``, i, had, to, stop, taking, zelapar, becaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7633</th>\n",
       "      <td>39201</td>\n",
       "      <td>Contrave</td>\n",
       "      <td>Weight Loss</td>\n",
       "      <td>\"61 adult male- adhd, 5 10;-215 lbs. heart dis...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>April 16, 2017</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>[``, 61, adult, male-, adhd, ,, 5, 10, ;, -215...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                          drugName            condition  \\\n",
       "7554    2855          Acetaminophen / pamabrom            Back Pain   \n",
       "4679  116091                OnabotulinumtoxinA    Overactive Bladde   \n",
       "4632   44044  Ethinyl estradiol / norgestimate        Birth Control   \n",
       "1450  169167                           Zelapar  Parkinson's Disease   \n",
       "7633   39201                          Contrave          Weight Loss   \n",
       "\n",
       "                                                 review  rating  \\\n",
       "7554  \"i found this while looking for acetaminophen ...    10.0   \n",
       "4679  \"had the botox for overactive bladder november...    10.0   \n",
       "4632  \"i switched from loestrin 24 fe to ortho tri-c...     8.0   \n",
       "1450  \"i had to stop taking zelapar because i was ha...     9.0   \n",
       "7633  \"61 adult male- adhd, 5 10;-215 lbs. heart dis...     9.0   \n",
       "\n",
       "                   date  usefulCount  rating_categorized  \\\n",
       "7554  February 13, 2017           13                   2   \n",
       "4679  December 23, 2015           48                   2   \n",
       "4632   January 11, 2012            0                   2   \n",
       "1450   January 19, 2009           30                   2   \n",
       "7633     April 16, 2017            8                   2   \n",
       "\n",
       "                                                     tk  \n",
       "7554  [``, i, found, this, while, looking, for, acet...  \n",
       "4679  [``, had, the, botox, for, overactive, bladder...  \n",
       "4632  [``, i, switched, from, loestrin, 24, fe, to, ...  \n",
       "1450  [``, i, had, to, stop, taking, zelapar, becaus...  \n",
       "7633  [``, 61, adult, male-, adhd, ,, 5, 10, ;, -215...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set mappings to convert from the Treebank tag set to WordNet tag set\n",
    "\n",
    "# WordNet POS tags are: NOUN = 'n', ADJ = 's', VERB = 'v', ADV = 'r', ADJ_SAT = 'a'\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "mappings = {\n",
    "        'CD':wn.NOUN, # cardinal number (one, two)                          \n",
    "        'EX':wn.ADV, # existential ‘there’ (there)                      \n",
    "        'IN':wn.ADV, # preposition/sub-conj (of, in, by)   \n",
    "        'JJ':wn.ADJ, # adjective (yellow)                  \n",
    "        'JJR':wn.ADJ, # adj., comparative (bigger)          \n",
    "        'JJS':wn.ADJ, # adj., superlative (wildest)                    \n",
    "        'NN':wn.NOUN, # noun, sing. or mass (llama)          \n",
    "        'NNS':wn.NOUN, # noun, plural (llamas)                  \n",
    "        'NNP':wn.NOUN, # proper noun, sing. (IBM)              \n",
    "        'NNPS':wn.NOUN, # proper noun, plural (Carolinas)\n",
    "        'PDT':wn.ADJ, # predeterminer (all, both)            \n",
    "        'RB':wn.ADV, # adverb (quickly, never)            \n",
    "        'RBR':wn.ADV, # adverb, comparative (faster)        \n",
    "        'RBS':wn.ADV, # adverb, superlative (fastest)     \n",
    "        'RP':wn.ADJ, # particle (up, off)\n",
    "        'VB':wn.VERB, # verb base form (eat)\n",
    "        'VBD':wn.VERB, # verb past tense (ate)\n",
    "        'VBG':wn.VERB, # verb gerund (eating)\n",
    "        'VBN':wn.VERB, # verb past participle (eaten)\n",
    "        'VBP':wn.VERB, # verb non-3sg pres (eat)\n",
    "        'VBZ':wn.VERB, # verb 3sg pres (eats)\n",
    "    }\n",
    "\n",
    "for key in mappings:\n",
    "    tag_map[key] = mappings[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.3 s, sys: 148 ms, total: 39.4 s\n",
      "Wall time: 39.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# tag part of speech to each word in every review.\n",
    "tagged = pos_tag_sents(data.tk)\n",
    "\n",
    "# remove stopwords and non-alpha words\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "tagged = [[word_tag for word_tag in review if (word_tag[0] not in cachedStopWords and word_tag[0].isalpha())] for review in tagged]\n",
    "\n",
    "# lemmatize words\n",
    "lemmatized = [[lemmatizer.lemmatize(word_tag[0], tag_map[word_tag[1]]) for word_tag in review] for review in tagged]\n",
    "\n",
    "# Save lemmatized words in \"text_final\" column\n",
    "data[\"text_final\"] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save processed data\n",
    "#data.to_csv(\"data_processed_10000.csv\")\n",
    "data = pd.read_csv(\"data_processed_10000.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets with a 20% test set percentage\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data['text_final'],data['rating_categorized'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels to integer values. Use this if labels are not already integers (ex. 'positive', ''negative', etc.)\n",
    "\n",
    "#Encoder = LabelEncoder()\n",
    "#Train_Y = Encoder.fit_transform(Train_Y)\n",
    "#Test_Y = Encoder.fit_transform(Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize word list using Term Frequency-Inverse Document Frequency statistic with a maximum of 5000 features.\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000, ngram_range=(2,4))\n",
    "train_vectors = Tfidf_vect.fit_transform(x_train)\n",
    "test_vectors = Tfidf_vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and initailize list of classifiers\n",
    "names = [\"Nearest Neighbors\", \n",
    "         \"SGDClassifier\",\n",
    "         \"Linear SVM\", \n",
    "         \"Random Forest\",\n",
    "         \"AdaBoost\",\n",
    "         \"Multinomial Naive Bayes\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(30, n_jobs=-1),\n",
    "    SGDClassifier(n_jobs=8, penalty=\"l2\"),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    RandomForestClassifier(max_depth=None, n_estimators=40, max_features=\"auto\", n_jobs=-1, verbose=0, warm_start=True),\n",
    "    AdaBoostClassifier(),\n",
    "    naive_bayes.MultinomialNB(alpha=1e-10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors : Accuracy Score ->  59.527  F1 Score ->  0.444\n",
      "SGDClassifier : Accuracy Score ->  66.918  F1 Score ->  0.63\n",
      "Linear SVM : Accuracy Score ->  59.527  F1 Score ->  0.444\n",
      "Random Forest : Accuracy Score ->  57.416  F1 Score ->  0.519\n",
      "AdaBoost : Accuracy Score ->  60.332  F1 Score ->  0.517\n",
      "Multinomial Naive Bayes : Accuracy Score ->  66.013  F1 Score ->  0.603\n",
      "CPU times: user 13.7 s, sys: 516 ms, total: 14.2 s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit all classifiers to training data and get accuracy score from test data\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    \n",
    "    # fit the training dataset on the classifier\n",
    "    clf.fit(train_vectors, y_train)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = clf.predict(test_vectors)\n",
    "    \n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    print(name,\": Accuracy Score -> \", round(accuracy_score(predictions, y_test)*100,3), \" F1 Score -> \",round(f1_score(y_test, predictions, average='weighted'),3))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 10,000 Data Points\n",
    "Nearest Neighbors : Accuracy Score ->  64.806\n",
    "SGDClassifier : Accuracy Score ->  70.94\n",
    "Linear SVM : Accuracy Score ->  60.985\n",
    "Random Forest : Accuracy Score ->  67.722\n",
    "AdaBoost : Accuracy Score ->  66.415\n",
    "Multinomial Naive Bayes : Accuracy Score ->  65.51\n",
    "\n",
    "# All Data Points\n",
    "Nearest Neighbors : Accuracy Score ->  60.663\n",
    "SGDClassifier : Accuracy Score ->  72.542\n",
    "Linear SVM : Accuracy Score ->  67.138\n",
    "Random Forest : Accuracy Score ->  59.906\n",
    "Multinomial Naive Bayes : Accuracy Score ->  68.448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use parfit library to optimize hyper-paraeters with parallelization\n",
    "\n",
    "paramGrid = ParameterGrid({\n",
    "    'min_samples_leaf': [1],#,3,5,10,15,25,50,100,125,150,175,200],\n",
    "    'max_features': ['sqrt'],#['sqrt', 'log2', 0.4, 0.5, 0.6, 0.7],\n",
    "    'n_estimators': [45],\n",
    "    'n_jobs': [-1],\n",
    "    'random_state': [42]\n",
    "})\n",
    "best_model, best_score, all_models, all_scores = pf.bestFit(RandomForestClassifier, paramGrid, \n",
    "     Train_X_Tfidf, Train_Y, Test_X_Tfidf, Test_Y, \n",
    "     metric=accuracy_score, scoreLabel='accuracy_score')\n",
    "print(best_model, best_score)\n",
    "#plotScores(all_scores, paramGrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=RandomForestClassifier(n_jobs=-1), n_jobs=-1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(RandomForestClassifier(n_jobs=-1), n_jobs=-1)\n",
    "clf.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.503 0.189 0.307]]\n"
     ]
    }
   ],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "c = make_pipeline(Tfidf_vect, clf)\n",
    "print(c.predict_proba([data.review[1]]).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=data.rating_categorized.unique())\n",
    "idx = 2\n",
    "class_names = [0,1,2]\n",
    "exp = explainer.explain_instance(data.review[idx], c.predict_proba, labels=[0, 1, 2])\n",
    "print('Document id: %d' % idx)\n",
    "print('Predicted class =', class_names[clf.predict(test_vectors[idx]).reshape(1,-1)[0,0]])\n",
    "print('True class: %s' % class_names[data.rating_categorized[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(data.review[idx], c.predict_proba, top_labels=3)\n",
    "print(exp.available_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment Analysis (1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "instance_type": "ml.t3.2xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
